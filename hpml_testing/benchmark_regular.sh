#!/bin/bash
  #SBATCH -p gpu-preempt
  #SBATCH -t 0:10:00
  #SBATCH --gpus-per-node=1
  #SBATCH --constraint=a100
  #SBATCH --mem=16G
  #SBATCH -o ../logs/slurm-%j.out
  #SBATCH -e ../logs/slurm-%j.err
  #SBATCH --job-name=regular_bench
  #SBATCH --nodes=1

  set -e

  MODEL_PATH="/datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08"
  TIMESTAMP=$(date +%Y%m%d_%H%M%S)
  RESULTS_DIR="benchmark_results"

  module load conda/latest
  conda activate context_parallelism
  module load cuda/12.6

  SUMMARY_CSV="$RESULTS_DIR/summary_regular_${TIMESTAMP}.csv"
  LOG_FILE="$RESULTS_DIR/run_regular_${TIMESTAMP}.log"

  SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
  export PYTHONPATH="$SCRIPT_DIR/../:$PYTHONPATH"

  echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
  nvidia-smi
  mkdir -p "$RESULTS_DIR"

  run_benchmark() {
      local num_tokens=$1

      echo "  Tokens: $num_tokens"
      python benchmark_regular.py \
          --architecture hf_pretrained \
          --variant 3.2-1b \
          --model_path "$MODEL_PATH" \
          --device_type cuda \
          --num_tokens $num_tokens \
          --num_decode_tokens 0 \
          --batch_size 1 \
          --dtype float16
  }

  echo "Regular Attention Benchmark - $(date)" | tee "$LOG_FILE"
  echo "Model: Llama 3.2-1B" | tee -a "$LOG_FILE"
  echo "" | tee -a "$LOG_FILE"

  # Test various context lengths
  for num_tokens in 256 512 1024 2048 4096 8192 16384 32768; do
      echo "Running: $num_tokens tokens" | tee -a "$LOG_FILE"
      run_benchmark $num_tokens 2>&1 | tee -a "$LOG_FILE"
      echo "" | tee -a "$LOG_FILE"
  done

  echo "" | tee -a "$LOG_FILE"
  echo "Benchmark complete!" | tee -a "$LOG_FILE"
  echo "Full log saved to: $LOG_FILE" | tee -a "$LOG_FILE"