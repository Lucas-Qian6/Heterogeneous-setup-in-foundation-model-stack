
@misc{attn_is_all,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{dao2022flashattentionfastmemoryefficientexact,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher RÃ©},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.14135}, 
}

@misc{rabe2022selfattentiondoesneedon2,
      title={Self-attention Does Not Need $O(n^2)$ Memory}, 
      author={Markus N. Rabe and Charles Staats},
      year={2022},
      eprint={2112.05682},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.05682}, 
}

@misc{liu2023blockwiseparalleltransformerlarge,
      title={Blockwise Parallel Transformer for Large Context Models}, 
      author={Hao Liu and Pieter Abbeel},
      year={2023},
      eprint={2305.19370},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.19370}, 
}

@misc{liu2023ringattentionblockwisetransformers,
      title={Ring Attention with Blockwise Transformers for Near-Infinite Context}, 
      author={Hao Liu and Matei Zaharia and Pieter Abbeel},
      year={2023},
      eprint={2310.01889},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.01889}, 
}

@ARTICLE{11150566,
  author={Yuan, Zhengyi and Wang, Xiong and Nie, Yuntao and Tao, Yufei and Li, Yuqing and Shao, Zhiyuan and Liao, Xiaofei and Li, Bo and Jin, Hai},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={DynPipe: Toward Dynamic End-to-End Pipeline Parallelism for Interference-Aware DNN Training}, 
  year={2025},
  volume={36},
  number={11},
  pages={2366-2382},
  keywords={Training;Computational modeling;Pipelines;Graphics processing units;Data models;Hardware;Convergence;Adaptation models;Interference;Processor scheduling;Pipelined training;dynamic computing environment;end-to-end performance;model re-partition},
  doi={10.1109/TPDS.2025.3605491}}

@inproceedings{chen2018architectural,
  title={An architectural framework for accelerating dynamic parallel algorithms on reconfigurable hardware},
  author={Chen, Tao and Srinath, Shreesha and Batten, Christopher and Suh, G Edward},
  booktitle={2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={55--67},
  year=2018,
  organization={IEEE}
}

@inproceedings{mei2025helix,
  title={Helix: Serving large language models over heterogeneous gpus and network via max-flow},
  author={Mei, Yixuan and Zhuang, Yonghao and Miao, Xupeng and Yang, Juncheng and Jia, Zhihao and Vinayak, Rashmi},
  booktitle={Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
  pages={586--602},
  year={2025}
}

@unpublished{IBM_FMS_RingAttention,
    author = {Gulcelik, Matthew, Lee},
    title = {Implementing Context Parallelism in IBM's FMS using Ring Attention},
    note = {2025 Spring Semester}
}

@misc{liquidweb2024gpu,
  author       = {Liquid Web},
  title        = {A100 vs H100 vs L40S: GPU comparison},
  year         = {2024},
  howpublished = {\url{https://www.liquidweb.com/gpu/a100-vs-h100-vs-l40s/}},
  note         = {Accessed: 2025-10-05}
}

@misc{pytorch_distributed,
  title        = {PyTorch Distributed Overview},
  author       = {{PyTorch Contributors}},
  howpublished = {\url{https://docs.pytorch.org/tutorials/beginner/dist_overview.html}},
  note         = {Accessed: 2025-10-05}
}

@article{liu2025cronus,
  title={Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill},
  author={Liu, Yunzhao and Xu, Qiang and Hu, Y Charlie},
  journal={arXiv preprint arXiv:2509.17357},
  year={2025}
}

@article{munkhdalai2024leave,
  title={Leave no context behind: Efficient infinite context transformers with infini-attention},
  author={Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  journal={arXiv preprint arXiv:2404.07143},
  volume={101},
  year={2024}
}

@article{acharya2024star,
  title={Star attention: Efficient llm inference over long sequences},
  author={Acharya, Shantanu and Jia, Fei and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2411.17116},
  year={2024}
}

@misc{foundation_model_stack,
  author       = {{IBM Research}},
  title        = {Foundation Model Stack (FMS)},
  howpublished = {\url{https://github.com/foundation-model-stack/foundation-model-stack}},
  year         = {2024},
  note         = {Accessed: 2025-12-01}
}

@misc{nvidia_mps_docs,
  author       = {{NVIDIA Corporation}},
  title        = {Multi-Process Service (MPS) Documentation},
  howpublished = {\url{https://docs.nvidia.com/deploy/mps/index.html}},
  year         = {2024},
  note         = {Accessed: 2025-12-01}
}

