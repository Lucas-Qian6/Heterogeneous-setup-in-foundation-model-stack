@inproceedings{attn_is_all,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{dao2022flashattentionfastmemoryefficientexact,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{rabe2022selfattentiondoesneedon2,
  title={Self-attention Does Not Need $O(n^2)$ Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021}
}

@article{liu2023blockwiseparalleltransformerlarge,
  title={Blockwise Parallel Transformer for Large Context Models},
  author={Liu, Hao and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{liu2023ringattentionblockwisetransformers,
  title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}
