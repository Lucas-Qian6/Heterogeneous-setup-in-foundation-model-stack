% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{attn_is_all}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{Advances in Neural Information Processing Systems}, vol.~30, 2017.

\bibitem{dao2022flashattentionfastmemoryefficientexact}
T.~Dao, D.~Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}, ``Flashattention: Fast and
  memory-efficient exact attention with io-awareness,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~35, pp. 16\,344--16\,359, 2022.

\bibitem{rabe2022selfattentiondoesneedon2}
M.~N. Rabe and C.~Staats, ``Self-attention does not need $o(n^2)$ memory,''
  \emph{arXiv preprint arXiv:2112.05682}, 2021.

\bibitem{liu2023blockwiseparalleltransformerlarge}
H.~Liu and P.~Abbeel, ``Blockwise parallel transformer for large context
  models,'' \emph{Advances in Neural Information Processing Systems}, vol.~36,
  2023.

\bibitem{liu2023ringattentionblockwisetransformers}
H.~Liu, M.~Zaharia, and P.~Abbeel, ``Ring attention with blockwise transformers
  for near-infinite context,'' \emph{arXiv preprint arXiv:2310.01889}, 2023.

\end{thebibliography}
